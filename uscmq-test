import faiss
import json
from fastapi import FastAPI
import numpy as np
import ollama
from sentence_transformers import SentenceTransformer
import yaml
import torch
import re

# Load configuration
with open('config.yaml', 'r') as config_file:
    config = yaml.safe_load(config_file)

index_path = config["INDEX_PATH"]
metadata_path = config["METADATA_PATH"]

# Load SentenceTransformer model
# model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
model.to(device)

index = faiss.read_index(index_path)  # Load FAISS index

# Load metadata and convert keys to integers
with open(metadata_path, "r", encoding="utf-8") as f:
    metadata = json.load(f)  # Load metadata


def search_faiss_index(query_text):
    print("Search in the index")
    query_embedding = model.encode([query_text], convert_to_numpy=True).astype(np.float32)
    distances, indices = index.search(query_embedding, k=3)
    results = []
    for i in indices[0]:  # Iterate over top-k results
        if i < len(metadata):
            results.append({
                "file_name": metadata[str(i)]["metadata"]["filename"],  # Convert i to string
                "text_chunk": metadata[str(i)]["text"],
                "url": metadata[str(i)]["metadata"].get("url", metadata[str(i)]["metadata"]["filename"])
                # Get URL if available
            })
    return results


def ask_llama(query, content):
    prompt = f"""
    You are an AI assistant. Summarize the given context **strictly within its content** without adding external information.

    **Context:**  
    {content}

    **User Query:**  
    {query}

    **Instructions:**  
    - Provide a **concise and precise summary** based **only** on the given context.  
    - **Do not** include unnecessary explanations, introductions, or phrases like "Based on the context".  
    - Keep the summary **direct and to the point** based on the query.  
    "Add some follow up questions like below based on the context and query":
    **Strictly provide in the below Output Format**
    `content`: <content>
    `follow-up-questions`: <follow-up-questions>
    """

    response = ollama.chat(
        model="llama3.2",
        messages=[{"role": "user", "content": prompt}],
        options={"temperature": 0.1}  # Set temperature here
    )
    # print(response)
    return response["message"]["content"]


app = FastAPI()


@app.post("/retrieval_and_generation/")
def main(query):
    # Example query
    # query = "What is diabetes?"
    # query = "What is z?"
    print("Query", query)
    search_results = search_faiss_index(query)
    retrieved_text = "\n\n".join([res["text_chunk"] for res in search_results])
    # Combine all chunks
    urls = set(res["url"] for res in search_results)
    summary = ask_llama(query, retrieved_text)
    print("Summary", summary)
    context_match = re.search(r"`content`:\s*(.*?)(\n\n|$)", summary, re.DOTALL)
    context = context_match.group(1).strip() if context_match else ""

    # Extract follow-up questions
    questions_match = re.search(r"`follow-up-questions`:\s*(.*)", summary, re.DOTALL)
    questions = re.findall(r"\d+\.\s(.*?\?)", questions_match.group(1)) if questions_match else []
    follow_up_questions = list(set(questions))
    return {
        "summary": context,
        "source": urls,
        "follow_up_questions": follow_up_questions}

    # print("Summary", summary)
    # print("Filenames",file_names)
